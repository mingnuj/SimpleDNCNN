{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNCNN_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMrFIEwrp7/j4UGw6ORk2Lq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c49c254ae8054a059d6997a544872977": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b660bfa5500d43e69967ddb7d636b4a4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_131411dc58ee4972aab1e49243e090d7",
              "IPY_MODEL_08f8e8b2f4bd44afbb3dd10e5bad1ff5"
            ]
          }
        },
        "b660bfa5500d43e69967ddb7d636b4a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "131411dc58ee4972aab1e49243e090d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_981d0d777d3e4bd1b1065ffe71b0a9c4",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_553f68791fce498da46a708c198c603f"
          }
        },
        "08f8e8b2f4bd44afbb3dd10e5bad1ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ed738a3b562e44a9840eff0592439ea9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1000/1000 [01:21&lt;00:00, 12.21it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dafc6196ada54109b08f5792cfad76d0"
          }
        },
        "981d0d777d3e4bd1b1065ffe71b0a9c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "553f68791fce498da46a708c198c603f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed738a3b562e44a9840eff0592439ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dafc6196ada54109b08f5792cfad76d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hm6CpCz7EN3"
      },
      "source": [
        "# Get Dataset from Google Drive\n",
        "please upload your dataset on google drive first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq2dWFwsXjfD",
        "outputId": "00952284-49df-461a-dfdf-4e4aea7652b8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0mdHDrEXtVo"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tqdm\n",
        "\n",
        "file_name = \"Multimedia_dataset.zip\"\n",
        "zip_path = os.path.join('/content/drive/MyDrive/Multimedia_dataset.zip')\n",
        "\n",
        "!cp \"{zip_path}\" .\n",
        "!unzip -q \"{file_name}\"\n",
        "!rm \"{file_name}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zgPrAP77OFS"
      },
      "source": [
        "# Noise Transform\n",
        "If you want to change how much noise you are giving, change the stddev and mean values at 'gaussian_noise' function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXSZARMfX3Wb"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "\n",
        "import random\n",
        "\n",
        "class NoiseTransform(object):\n",
        "  def __init__(self, size=180, mode=\"training\"):\n",
        "    super(NoiseTransform, self).__init__()\n",
        "    self.size = size\n",
        "    self.mode = mode\n",
        "  \n",
        "  def gaussian_noise(self, img):\n",
        "    mean = 0\n",
        "    stddev = 25\n",
        "    noise = Variable(torch.zeros(img.size()))\n",
        "    noise = noise.data.normal_(mean, stddev/255.)\n",
        "\n",
        "    return noise\n",
        "\n",
        "  def __call__(self, img):\n",
        "    if (self.mode == \"training\") | (self.mode == \"validation\"):\n",
        "      self.gt_transform = transforms.Compose([\n",
        "        # transforms.RandomCrop(self.size),\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor()])\n",
        "      self.noise_transform = transforms.Compose([\n",
        "        # transforms.RandomCrop(self.size),\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(self.gaussian_noise),\n",
        "      ])\n",
        "      return self.gt_transform(img), self.noise_transform(img)\n",
        "\n",
        "    elif self.mode == \"testing\":\n",
        "      self.gt_transform = transforms.Compose([\n",
        "        transforms.Resize((self.size, self.size), interpolation=2),\n",
        "        transforms.ToTensor()])\n",
        "      return self.gt_transform(img)\n",
        "    else:\n",
        "      return NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xUqNMXs7S1G"
      },
      "source": [
        "#Dataloader for Noise Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dnPzzwDX_aL"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data  as data\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "class NoiseDataset(data.Dataset):\n",
        "  def __init__(self, root_path, size):\n",
        "    super(NoiseDataset, self).__init__()\n",
        "\n",
        "    self.root_path = root_path\n",
        "    self.size = size\n",
        "    self.transforms = None\n",
        "    self.examples = None\n",
        "\n",
        "  def set_mode(self, mode):\n",
        "    self.mode = mode\n",
        "    self.transforms = NoiseTransform(self.size, mode)\n",
        "    if mode == \"training\":\n",
        "      train_dir = os.path.join(self.root_path, \"train\")\n",
        "      self.examples = [os.path.join(self.root_path, \"train\", dirs) for dirs in os.listdir(train_dir)]\n",
        "    elif mode == \"validation\":\n",
        "      val_dir = os.path.join(self.root_path, \"validation\")\n",
        "      self.examples = [os.path.join(self.root_path, \"validation\", dirs) for dirs in os.listdir(val_dir)]\n",
        "    elif mode == \"testing\":\n",
        "      test_dir = os.path.join(self.root_path, \"test\")\n",
        "      self.examples = [os.path.join(self.root_path, \"test\", dirs) for dirs in os.listdir(test_dir)]\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.examples)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    file_name = self.examples[idx]\n",
        "    image = Image.open(file_name)\n",
        "\n",
        "    if self.mode == \"testing\":\n",
        "      input_img = self.transforms(image)\n",
        "      sample = {\"img\": input_img, \"file_name\": self.examples[idx].split(\"/\")[-1]}\n",
        "    else:\n",
        "      clean, noise = self.transforms(image)\n",
        "      sample = {\"img\": clean, \"noise\": noise, \"file_name\": self.examples[idx].split(\"/\")[-1]}\n",
        "\n",
        "    return sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvG2AvtD7ZNN"
      },
      "source": [
        "# Network\n",
        "Simplified DNCNN network for example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI9M89MfYxNg"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class DNCNN(nn.Module):\n",
        "  def __init__(self, in_planes=3, blocks=17, hidden=64, kernel_size=3, padding=1, bias=False):\n",
        "    super(DNCNN, self).__init__()\n",
        "    self.conv_f = nn.Conv2d(in_channels=in_planes, out_channels=hidden, kernel_size=kernel_size, padding=padding, bias=bias)\n",
        "    self.conv_h = nn.Conv2d(in_channels=hidden, out_channels=hidden, kernel_size=kernel_size, padding=padding, bias=bias)\n",
        "    self.conv_l = nn.Conv2d(in_channels=hidden, out_channels=in_planes, kernel_size=kernel_size, padding=padding, bias=bias)\n",
        "\n",
        "    self.bn = nn.BatchNorm2d(hidden)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    self.hidden_layer = self.mk_hidden_layer(blocks)\n",
        "\n",
        "  def mk_hidden_layer(self, blocks=17):\n",
        "    layers = []\n",
        "    for _ in range(blocks-2):\n",
        "      layers.append(self.conv_h)\n",
        "      layers.append(self.bn)\n",
        "      layers.append(self.relu)\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = self.conv_f(x)\n",
        "    out = self.relu(out)\n",
        "\n",
        "    out = self.hidden_layer(out)\n",
        "\n",
        "    out = self.conv_l(out)\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUzUSRtE7pJd"
      },
      "source": [
        "# Tensorboard\n",
        "For training progress visualization. Run before training phase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fclbhRXFEr7A",
        "outputId": "b495f834-e232-4723-eaa2-04c3cc48af1d"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\r\u001b[K     |██▊                             | 10kB 21.6MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20kB 19.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30kB 10.9MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 51kB 7.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 61kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71kB 8.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 92kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 102kB 7.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 112kB 7.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 7.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (56.1.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHskMP7aDia2"
      },
      "source": [
        "# %load_ext tensorboard\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_8NDgIPDpzf"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdiW7hUd8AS9"
      },
      "source": [
        "# Training Phase\n",
        "Simple DNCNN training code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIuE0exoYAcx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data  as data\n",
        "\n",
        "from torchvision import transforms\n",
        "from tensorboardX import SummaryWriter\n",
        "import torchvision.utils as tvutils\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tqdm.notebook as tq\n",
        "from PIL import Image\n",
        "from skimage.measure.simple_metrics import compare_psnr\n",
        "\n",
        "def batch_PSNR(img, imclean, data_range):\n",
        "    Img = img.data.cpu().numpy().astype(np.float32)\n",
        "    Iclean = imclean.data.cpu().numpy().astype(np.float32)\n",
        "    PSNR = 0\n",
        "    for i in range(Img.shape[0]):\n",
        "        PSNR += compare_psnr(Iclean[i, :, :, :], Img[i, :, :, :], data_range=data_range)\n",
        "    return (PSNR/Img.shape[0])\n",
        "\n",
        "# Change to your data root directory\n",
        "root_path = \"/content/\"\n",
        "save_path = \"/content/drive/MyDrive/DNCNN_models\"\n",
        "\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "# Dataloader setting\n",
        "train_dataset = NoiseDataset(root_path, 128)\n",
        "train_dataset.set_mode(\"training\")\n",
        "\n",
        "val_dataset = NoiseDataset(root_path, 128)\n",
        "val_dataset.set_mode(\"validation\")\n",
        "\n",
        "train_dataloader = data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_dataloader = data.DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Model declaration\n",
        "net = DNCNN()\n",
        "model = nn.DataParallel(net)\n",
        "\n",
        "# loss\n",
        "criterion = nn.MSELoss(size_average=False)\n",
        "\n",
        "if use_cuda:\n",
        "  model.to('cuda')\n",
        "  criterion.to('cuda')\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, eps=1e-08)\n",
        "\n",
        "step = 0\n",
        "# Tensorboard writer\n",
        "writer = SummaryWriter(\"logs\")\n",
        "\n",
        "for epoch in range(20):\n",
        "  print('Epoch {}/{}'.format(epoch + 1, 50))\n",
        "  print('-' * 10)\n",
        "\n",
        "  for i, data in enumerate(tq.tqdm(train_dataloader)):\n",
        "    if use_cuda:\n",
        "      img = data[\"img\"].to('cuda')\n",
        "      noise = data[\"noise\"].to('cuda')\n",
        "    \n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    noise_image = img + noise\n",
        "    model_input = torch.clamp(noise_image, 0, 1)\n",
        "\n",
        "    preds = model(model_input)\n",
        "    \n",
        "    loss = criterion(preds, noise) / (model_input.size()[0] * 2)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    out_train = torch.clamp(model_input - model(model_input), 0., 1.)\n",
        "    psnr_train = batch_PSNR(out_train, model_input, 1.)\n",
        "\n",
        "    if step % 100 == 0:\n",
        "      # Log the scalar values\n",
        "      writer.add_scalar('loss', loss.item(), step)\n",
        "      writer.add_scalar('PSNR on training data', psnr_train, step)\n",
        "\n",
        "      # log the images => Tensorboard\n",
        "      Img = tvutils.make_grid(img.data, nrow=4, normalize=True, scale_each=True)\n",
        "      Imgn = tvutils.make_grid(model_input.data, nrow=4, normalize=True, scale_each=True)\n",
        "      Irecon = tvutils.make_grid(out_train.data, nrow=4, normalize=True, scale_each=True)\n",
        "      writer.add_image('clean image', Img, epoch)\n",
        "      writer.add_image('noisy image', Imgn, epoch)\n",
        "      writer.add_image('reconstructed image', Irecon, epoch)\n",
        "      print(\"[epoch %d][%d/%d] loss: %.4f PSNR_train: %.4f\" %\n",
        "          (epoch + 1, i + 1, len(train_dataloader), loss.item(), psnr_train))\n",
        "    step += 1\n",
        "    \n",
        "  torch.save(model.module.state_dict(), os.path.join(save_path, \"{}.tar\".format(epoch+30)))\n",
        "  print(\"saved at {}\".format(os.path.join(save_path, \"{}.tar\".format(epoch+30))))\n",
        "\n",
        "  psnr_val = 0\n",
        "\n",
        "  # Validation on training phase\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for val_data in tq.tqdm(val_dataloader):\n",
        "      img = val_data[\"img\"].to('cuda')\n",
        "      noise = val_data[\"noise\"].to('cuda')\n",
        "\n",
        "      val_image = img + noise\n",
        "      preds = model(val_image)\n",
        "\n",
        "      out_val = torch.clamp(val_image - preds, 0., 1.)\n",
        "      psnr_val += batch_PSNR(out_val, val_image, 1.)\n",
        "\n",
        "      val_Img = tvutils.make_grid(img.data, nrow=4, normalize=True, scale_each=True)\n",
        "      val_Imgn = tvutils.make_grid(val_image.data, nrow=4, normalize=True, scale_each=True)\n",
        "      val_Irecon = tvutils.make_grid(out_val.data, nrow=4, normalize=True, scale_each=True)\n",
        "      writer.add_image('clean image', val_Img, epoch)\n",
        "      writer.add_image('noisy image', val_Imgn, epoch)\n",
        "      writer.add_image('validation reconstructed image', val_Irecon, epoch)\n",
        "    \n",
        "    psnr_val /= len(val_dataloader)\n",
        "    print(\"\\n[epoch %d] PSNR_val: %.4f\" % (epoch + 1, psnr_val))\n",
        "    writer.add_scalar('PSNR on validation data', psnr_val, epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139,
          "referenced_widgets": [
            "c49c254ae8054a059d6997a544872977",
            "b660bfa5500d43e69967ddb7d636b4a4",
            "131411dc58ee4972aab1e49243e090d7",
            "08f8e8b2f4bd44afbb3dd10e5bad1ff5",
            "981d0d777d3e4bd1b1065ffe71b0a9c4",
            "553f68791fce498da46a708c198c603f",
            "ed738a3b562e44a9840eff0592439ea9",
            "dafc6196ada54109b08f5792cfad76d0"
          ]
        },
        "id": "KRXBdu0mURuc",
        "outputId": "51174bb8-3ece-40fe-d6ca-2d427e817c11"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data  as data\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tqdm.notebook as tq\n",
        "from PIL import Image\n",
        "from skimage.measure.simple_metrics import compare_psnr\n",
        "\n",
        "def image_save(img, path):\n",
        "  if isinstance(img, torch.Tensor):\n",
        "    img = transforms.ToPILImage()(img)\n",
        "  img.save(path)\n",
        "\n",
        "def batch_PSNR(img, imclean, data_range):\n",
        "    Img = img.data.cpu().numpy().astype(np.float32)\n",
        "    Iclean = imclean.data.cpu().numpy().astype(np.float32)\n",
        "    PSNR = 0\n",
        "    for i in range(Img.shape[0]):\n",
        "        PSNR += compare_psnr(Iclean[i, :, :, :], Img[i, :, :, :], data_range=data_range)\n",
        "    return (PSNR/Img.shape[0])\n",
        "\n",
        "# Change to your data root directory\n",
        "image_path = \"/content/drive/MyDrive/Multimedia_test_dataset/denoising2/\"\n",
        "checkpoint_path = \"/content/drive/MyDrive/DNCNN_models/37.tar\"\n",
        "result_save_path = \"/content/drive/MyDrive/Multimedia_test_dataset/dncnn_test_result\"\n",
        "\n",
        "# Depend on runtime setting\n",
        "use_cuda = True\n",
        "\n",
        "test_dataset = NoiseDataset(image_path, 128)\n",
        "test_dataset.set_mode(\"testing\")\n",
        "\n",
        "test_dataloader = data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "net = DNCNN()\n",
        "\n",
        "if use_cuda:\n",
        "  net.to('cuda')\n",
        "\n",
        "net.load_state_dict(torch.load(checkpoint_path))\n",
        "model = nn.DataParallel(net)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "psnr_test = 0\n",
        "\n",
        "for i, data in enumerate(tq.tqdm(test_dataloader)):\n",
        "  if use_cuda:\n",
        "    img = data[\"img\"].to('cuda')\n",
        "  file_name = data[\"file_name\"]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    out_test = torch.clamp(img - model(img), 0., 1.)\n",
        "    psnr = batch_PSNR(out_test, img, 1.)\n",
        "    psnr_test += psnr\n",
        "    for idx in range(len(img)):\n",
        "      image_save(out_test[idx], os.path.join(result_save_path, file_name[idx]))\n",
        "\n",
        "psnr_test /= len(test_dataloader)\n",
        "print(\"\\nPSNR on test data %f\" % psnr_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c49c254ae8054a059d6997a544872977",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: DEPRECATED: skimage.measure.compare_psnr has been moved to skimage.metrics.peak_signal_noise_ratio. It will be removed from skimage.measure in version 0.18.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "PSNR on test data 30.914452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5J3K3Vfbhmx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}